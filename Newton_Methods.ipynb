{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "QNqjxImE0dYX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d2c2c9e-8c00-436f-e0c4-fa7bfc835be2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "# Useful starting lines\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "if IN_COLAB:\n",
        "    # Clone the entire repo to access the files.\n",
        "    !git clone -l -s https://github.com/epfml/OptML_course.git cloned-repo\n",
        "    %cd cloned-repo/labs/ex05/template/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plgurRWO0hCM",
        "outputId": "751c16ff-eed5-4b8c-bc38-2678dfa6aa53"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cloned-repo'...\n",
            "warning: --local is ignored\n",
            "remote: Enumerating objects: 2259, done.\u001b[K\n",
            "remote: Counting objects: 100% (423/423), done.\u001b[K\n",
            "remote: Compressing objects: 100% (219/219), done.\u001b[K\n",
            "remote: Total 2259 (delta 208), reused 403 (delta 195), pack-reused 1836\u001b[K\n",
            "Receiving objects: 100% (2259/2259), 425.21 MiB | 25.72 MiB/s, done.\n",
            "Resolving deltas: 100% (1112/1112), done.\n",
            "/content/cloned-repo/labs/ex05/template/cloned-repo/labs/ex05/template/cloned-repo/labs/ex05/template\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "from helpers import *\n",
        "\n",
        "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
        "x, mean_x, std_x = standardize(height)\n",
        "b, A = build_model_data(x, weight)"
      ],
      "metadata": {
        "id": "NOtxBTya0ltY"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def full_objective(targets_b, data_A, params_x):\n",
        "    \"\"\"Compute the least squares objective over the whole dataset\"\"\"\n",
        "    return 0.5 * np.mean(((data_A @ params_x) - targets_b)**2)"
      ],
      "metadata": {
        "id": "Lldo8wSezUbP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Damped Newton"
      ],
      "metadata": {
        "id": "7kSqIPF4Lai_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step of damped Newton\n",
        "def damped_newton_update(targets_b, data_A, params_x, lambda_param):\n",
        "\n",
        "  batch_size = len(targets_b)\n",
        "  hessian = data_A.T@data_A\n",
        "  grad = -data_A.T.dot(targets_b - data_A.dot(params_x)) / batch_size\n",
        "  dx = np.linalg.solve(hessian + lambda_param*np.identity(hessian.shape[0]), grad)\n",
        "\n",
        "  return dx"
      ],
      "metadata": {
        "id": "S9tHWlv60q0d"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parameters of the algorithm.\n",
        "max_iters = int(100)  # 10 passes through the dataset\n",
        "batch_size = 20\n",
        "\n",
        "# Initialization\n",
        "x_initial = np.zeros(A.shape[1])\n",
        "\n",
        "# Start SGD.\n",
        "start_time = datetime.datetime.now()\n",
        "damped_newton_objectives, damped_newton_xs = damped_newton(\n",
        "    b, A, x_initial, batch_size, max_iters)\n",
        "end_time = datetime.datetime.now()\n",
        "\n",
        "# Print result\n",
        "exection_time = (end_time - start_time).total_seconds()\n",
        "print(\"Damped_newton: execution time={t:.3f} seconds\".format(t=exection_time))"
      ],
      "metadata": {
        "id": "oGITHvfd-oBW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a5be230-48c6-47b5-cde6-3b8638e88fd3"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Damped_newton(0000/0099): objective =    2521.20\n",
            "Damped_newton(0001/0099): objective =    2271.47\n",
            "Damped_newton(0002/0099): objective =    2050.37\n",
            "Damped_newton(0003/0099): objective =    1844.44\n",
            "Damped_newton(0004/0099): objective =    1666.12\n",
            "Damped_newton(0005/0099): objective =    1499.44\n",
            "Damped_newton(0006/0099): objective =    1352.66\n",
            "Damped_newton(0007/0099): objective =    1222.43\n",
            "Damped_newton(0008/0099): objective =    1105.37\n",
            "Damped_newton(0009/0099): objective =     995.87\n",
            "Damped_newton(0010/0099): objective =     899.68\n",
            "Damped_newton(0011/0099): objective =     810.49\n",
            "Damped_newton(0012/0099): objective =     731.62\n",
            "Damped_newton(0013/0099): objective =     666.44\n",
            "Damped_newton(0014/0099): objective =     605.38\n",
            "Damped_newton(0015/0099): objective =     546.46\n",
            "Damped_newton(0016/0099): objective =     494.46\n",
            "Damped_newton(0017/0099): objective =     445.92\n",
            "Damped_newton(0018/0099): objective =     401.86\n",
            "Damped_newton(0019/0099): objective =     363.87\n",
            "Damped_newton(0020/0099): objective =     329.93\n",
            "Damped_newton(0021/0099): objective =     298.64\n",
            "Damped_newton(0022/0099): objective =     270.85\n",
            "Damped_newton(0023/0099): objective =     246.66\n",
            "Damped_newton(0024/0099): objective =     223.08\n",
            "Damped_newton(0025/0099): objective =     203.69\n",
            "Damped_newton(0026/0099): objective =     186.31\n",
            "Damped_newton(0027/0099): objective =     167.32\n",
            "Damped_newton(0028/0099): objective =     149.94\n",
            "Damped_newton(0029/0099): objective =     138.49\n",
            "Damped_newton(0030/0099): objective =     127.92\n",
            "Damped_newton(0031/0099): objective =     116.81\n",
            "Damped_newton(0032/0099): objective =     107.03\n",
            "Damped_newton(0033/0099): objective =      98.44\n",
            "Damped_newton(0034/0099): objective =      89.72\n",
            "Damped_newton(0035/0099): objective =      84.12\n",
            "Damped_newton(0036/0099): objective =      77.15\n",
            "Damped_newton(0037/0099): objective =      70.46\n",
            "Damped_newton(0038/0099): objective =      65.61\n",
            "Damped_newton(0039/0099): objective =      60.98\n",
            "Damped_newton(0040/0099): objective =      56.76\n",
            "Damped_newton(0041/0099): objective =      53.49\n",
            "Damped_newton(0042/0099): objective =      50.03\n",
            "Damped_newton(0043/0099): objective =      46.01\n",
            "Damped_newton(0044/0099): objective =      43.44\n",
            "Damped_newton(0045/0099): objective =      41.73\n",
            "Damped_newton(0046/0099): objective =      39.62\n",
            "Damped_newton(0047/0099): objective =      37.42\n",
            "Damped_newton(0048/0099): objective =      35.16\n",
            "Damped_newton(0049/0099): objective =      33.14\n",
            "Damped_newton(0050/0099): objective =      31.07\n",
            "Damped_newton(0051/0099): objective =      29.34\n",
            "Damped_newton(0052/0099): objective =      28.22\n",
            "Damped_newton(0053/0099): objective =      26.99\n",
            "Damped_newton(0054/0099): objective =      25.68\n",
            "Damped_newton(0055/0099): objective =      24.57\n",
            "Damped_newton(0056/0099): objective =      23.62\n",
            "Damped_newton(0057/0099): objective =      22.39\n",
            "Damped_newton(0058/0099): objective =      21.72\n",
            "Damped_newton(0059/0099): objective =      21.00\n",
            "Damped_newton(0060/0099): objective =      20.26\n",
            "Damped_newton(0061/0099): objective =      19.69\n",
            "Damped_newton(0062/0099): objective =      19.53\n",
            "Damped_newton(0063/0099): objective =      19.17\n",
            "Damped_newton(0064/0099): objective =      18.84\n",
            "Damped_newton(0065/0099): objective =      18.45\n",
            "Damped_newton(0066/0099): objective =      18.26\n",
            "Damped_newton(0067/0099): objective =      18.07\n",
            "Damped_newton(0068/0099): objective =      17.58\n",
            "Damped_newton(0069/0099): objective =      17.49\n",
            "Damped_newton(0070/0099): objective =      17.46\n",
            "Damped_newton(0071/0099): objective =      17.39\n",
            "Damped_newton(0072/0099): objective =      17.17\n",
            "Damped_newton(0073/0099): objective =      17.15\n",
            "Damped_newton(0074/0099): objective =      17.17\n",
            "Damped_newton(0075/0099): objective =      16.91\n",
            "Damped_newton(0076/0099): objective =      16.91\n",
            "Damped_newton(0077/0099): objective =      16.80\n",
            "Damped_newton(0078/0099): objective =      16.93\n",
            "Damped_newton(0079/0099): objective =      16.62\n",
            "Damped_newton(0080/0099): objective =      16.42\n",
            "Damped_newton(0081/0099): objective =      16.25\n",
            "Damped_newton(0082/0099): objective =      16.21\n",
            "Damped_newton(0083/0099): objective =      16.24\n",
            "Damped_newton(0084/0099): objective =      16.09\n",
            "Damped_newton(0085/0099): objective =      16.02\n",
            "Damped_newton(0086/0099): objective =      16.07\n",
            "Damped_newton(0087/0099): objective =      16.05\n",
            "Damped_newton(0088/0099): objective =      15.92\n",
            "Damped_newton(0089/0099): objective =      15.84\n",
            "Damped_newton(0090/0099): objective =      15.80\n",
            "Damped_newton(0091/0099): objective =      15.72\n",
            "Damped_newton(0092/0099): objective =      15.75\n",
            "Damped_newton(0093/0099): objective =      15.81\n",
            "Damped_newton(0094/0099): objective =      15.84\n",
            "Damped_newton(0095/0099): objective =      15.71\n",
            "Damped_newton(0096/0099): objective =      15.76\n",
            "Damped_newton(0097/0099): objective =      15.71\n",
            "Damped_newton(0098/0099): objective =      15.68\n",
            "Damped_newton(0099/0099): objective =      15.63\n",
            "Damped_newton: execution time=0.121 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def damped_newton(targets_b, data_A, initial_x, batch_size, max_iters):\n",
        "\n",
        "  #Hyperparameter for damped newton\n",
        "  lambda_param = 0.01\n",
        "\n",
        "  xs = [initial_x]\n",
        "  objectives = []\n",
        "  x = initial_x\n",
        "\n",
        "  for iteration in range(max_iters):\n",
        "\n",
        "    indices = np.random.choice(len(targets_b), batch_size, replace=False)\n",
        "    dx = damped_newton_update(targets_b[indices],data_A[indices],x,lambda_param)\n",
        "\n",
        "    x = x - dx\n",
        "\n",
        "    xs.append(x.copy())\n",
        "    objective = full_objective(targets_b, data_A, x)\n",
        "    objectives.append(objective)\n",
        "\n",
        "    print(\"Damped_newton({bi:04d}/{ti:04d}): objective = {l:10.2f}\".format(\n",
        "                  bi=iteration, ti=max_iters - 1, l=objective))\n",
        "  return objectives, xs\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lxpJCGHG4omb"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Secant"
      ],
      "metadata": {
        "id": "D1ACbltbL9QK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def secant_update(targets_b, data_A, params_x, lambda_param, params_x_prev, hessian_inv_prev):\n",
        "\n",
        "  batch_size = len(targets_b)\n",
        "\n",
        "  grad = -data_A.T.dot(targets_b - data_A.dot(params_x)) / batch_size\n",
        "  grad_prev = -data_A.T.dot(targets_b - data_A.dot(params_x_prev)) / batch_size\n",
        "\n",
        "  sigma = params_x - params_x_prev\n",
        "  y = grad - grad_prev\n",
        "  H = hessian_inv_prev\n",
        "\n",
        "  sigma = sigma.reshape((sigma.shape[0],1))\n",
        "  y = y.reshape((y.shape[0],1))\n",
        "\n",
        "  E = (1/y.T.dot(sigma))*((1 + (1/y.T.dot(sigma))*(y.T.dot(H.dot(y))))*(sigma.dot(sigma.T)) - H.dot(y.dot(sigma.T)) - sigma.dot(y.T.dot(H)))\n",
        "  H_1 = H + E\n",
        "  dx = H_1.dot(grad)\n",
        "\n",
        "  return dx, H_1\n"
      ],
      "metadata": {
        "id": "6IzRMwqB2M-L"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def secant(targets_b, data_A, initial_x, batch_size, max_iters):\n",
        "\n",
        "  xs = [initial_x]\n",
        "  objectives = []\n",
        "  x = initial_x\n",
        "\n",
        "  H = np.linalg.inv(data_A.T@data_A)\n",
        "  x = np.random.rand(x.shape[0])\n",
        "  xs.append(x.copy())\n",
        "  objective = full_objective(targets_b, data_A, x)\n",
        "  objectives.append(objective)\n",
        "\n",
        "\n",
        "  for iteration in range(max_iters):\n",
        "\n",
        "    indices = np.random.choice(len(targets_b), batch_size, replace=False)\n",
        "    dx, H = secant_update(targets_b[indices],data_A[indices],x,0,xs[-2],H)\n",
        "    x = x - dx\n",
        "\n",
        "    xs.append(x.copy())\n",
        "    objective = full_objective(targets_b, data_A, x)\n",
        "    objectives.append(objective)\n",
        "\n",
        "    print(\"Secant({bi:04d}/{ti:04d}): objective = {l:10.2f}\".format(\n",
        "                  bi=iteration, ti=max_iters - 1, l=objective))\n",
        "  return objectives, xs"
      ],
      "metadata": {
        "id": "8tjD2x3jh0Tk"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parameters of the algorithm.\n",
        "max_iters = int(100)  # 10 passes through the dataset\n",
        "batch_size = 20\n",
        "\n",
        "# Initialization\n",
        "x_initial = np.zeros(A.shape[1])\n",
        "\n",
        "# Start SGD.\n",
        "start_time = datetime.datetime.now()\n",
        "secant_objectives, secant_xs = secant(\n",
        "    b, A, x_initial, batch_size, max_iters)\n",
        "end_time = datetime.datetime.now()\n",
        "\n",
        "# Print result\n",
        "exection_time = (end_time - start_time).total_seconds()\n",
        "print(\"Secant: execution time={t:.3f} seconds\".format(t=exection_time))"
      ],
      "metadata": {
        "id": "2p0mntWEpzKd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed3d25be-6ea2-4d85-e2b3-5ad5e14f83e7"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Secant(0000/0099): objective =      88.97\n",
            "Secant(0001/0099): objective =      88.52\n",
            "Secant(0002/0099): objective =     103.84\n",
            "Secant(0003/0099): objective =      96.39\n",
            "Secant(0004/0099): objective =      98.93\n",
            "Secant(0005/0099): objective =      88.68\n",
            "Secant(0006/0099): objective =      88.31\n",
            "Secant(0007/0099): objective =      90.25\n",
            "Secant(0008/0099): objective =      88.25\n",
            "Secant(0009/0099): objective =      90.17\n",
            "Secant(0010/0099): objective =      89.60\n",
            "Secant(0011/0099): objective =      88.10\n",
            "Secant(0012/0099): objective =      88.11\n",
            "Secant(0013/0099): objective =      88.09\n",
            "Secant(0014/0099): objective =      88.19\n",
            "Secant(0015/0099): objective =      89.52\n",
            "Secant(0016/0099): objective =      88.54\n",
            "Secant(0017/0099): objective =      88.95\n",
            "Secant(0018/0099): objective =      88.12\n",
            "Secant(0019/0099): objective =      89.07\n",
            "Secant(0020/0099): objective =      92.05\n",
            "Secant(0021/0099): objective =      88.22\n",
            "Secant(0022/0099): objective =      89.26\n",
            "Secant(0023/0099): objective =      88.00\n",
            "Secant(0024/0099): objective =      92.62\n",
            "Secant(0025/0099): objective =      87.96\n",
            "Secant(0026/0099): objective =      97.48\n",
            "Secant(0027/0099): objective =      89.23\n",
            "Secant(0028/0099): objective =      96.24\n",
            "Secant(0029/0099): objective =      88.92\n",
            "Secant(0030/0099): objective =      89.77\n",
            "Secant(0031/0099): objective =      90.34\n",
            "Secant(0032/0099): objective =      87.96\n",
            "Secant(0033/0099): objective =      90.80\n",
            "Secant(0034/0099): objective =      90.39\n",
            "Secant(0035/0099): objective =      87.70\n",
            "Secant(0036/0099): objective =      87.32\n",
            "Secant(0037/0099): objective =      88.50\n",
            "Secant(0038/0099): objective =      89.27\n",
            "Secant(0039/0099): objective =      87.43\n",
            "Secant(0040/0099): objective =      87.45\n",
            "Secant(0041/0099): objective =      86.80\n",
            "Secant(0042/0099): objective =      88.31\n",
            "Secant(0043/0099): objective =      85.98\n",
            "Secant(0044/0099): objective =      88.74\n",
            "Secant(0045/0099): objective =      85.48\n",
            "Secant(0046/0099): objective =      86.15\n",
            "Secant(0047/0099): objective =      90.48\n",
            "Secant(0048/0099): objective =      86.42\n",
            "Secant(0049/0099): objective =      85.62\n",
            "Secant(0050/0099): objective =      91.89\n",
            "Secant(0051/0099): objective =      85.53\n",
            "Secant(0052/0099): objective =      93.12\n",
            "Secant(0053/0099): objective =      83.08\n",
            "Secant(0054/0099): objective =      84.79\n",
            "Secant(0055/0099): objective =      81.59\n",
            "Secant(0056/0099): objective =      28.22\n",
            "Secant(0057/0099): objective =      19.38\n",
            "Secant(0058/0099): objective =      15.97\n",
            "Secant(0059/0099): objective =      15.76\n",
            "Secant(0060/0099): objective =      17.94\n",
            "Secant(0061/0099): objective =      20.41\n",
            "Secant(0062/0099): objective =      24.98\n",
            "Secant(0063/0099): objective =      17.67\n",
            "Secant(0064/0099): objective =      18.54\n",
            "Secant(0065/0099): objective =      16.08\n",
            "Secant(0066/0099): objective =      15.43\n",
            "Secant(0067/0099): objective =      15.61\n",
            "Secant(0068/0099): objective =      15.66\n",
            "Secant(0069/0099): objective =      18.60\n",
            "Secant(0070/0099): objective =      15.69\n",
            "Secant(0071/0099): objective =      17.70\n",
            "Secant(0072/0099): objective =      15.62\n",
            "Secant(0073/0099): objective =      16.03\n",
            "Secant(0074/0099): objective =      16.89\n",
            "Secant(0075/0099): objective =      16.95\n",
            "Secant(0076/0099): objective =      15.87\n",
            "Secant(0077/0099): objective =      16.38\n",
            "Secant(0078/0099): objective =      17.74\n",
            "Secant(0079/0099): objective =      19.24\n",
            "Secant(0080/0099): objective =      15.87\n",
            "Secant(0081/0099): objective =      18.21\n",
            "Secant(0082/0099): objective =      16.97\n",
            "Secant(0083/0099): objective =      17.17\n",
            "Secant(0084/0099): objective =      18.17\n",
            "Secant(0085/0099): objective =      16.98\n",
            "Secant(0086/0099): objective =      15.82\n",
            "Secant(0087/0099): objective =      16.09\n",
            "Secant(0088/0099): objective =      18.71\n",
            "Secant(0089/0099): objective =      16.92\n",
            "Secant(0090/0099): objective =      15.69\n",
            "Secant(0091/0099): objective =      17.09\n",
            "Secant(0092/0099): objective =      16.03\n",
            "Secant(0093/0099): objective =      15.94\n",
            "Secant(0094/0099): objective =      19.72\n",
            "Secant(0095/0099): objective =      19.81\n",
            "Secant(0096/0099): objective =      15.70\n",
            "Secant(0097/0099): objective =      16.24\n",
            "Secant(0098/0099): objective =      16.92\n",
            "Secant(0099/0099): objective =      15.83\n",
            "Secant: execution time=0.107 seconds\n"
          ]
        }
      ]
    }
  ]
}